{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f6fa32-fc5b-4cef-8652-2a2de8668dc4",
   "metadata": {},
   "source": [
    "# Built a simple bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb312c5-8b7c-411a-bbd6-eb43d1053510",
   "metadata": {},
   "source": [
    "## How to integrate LLM into our Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3e11e8-b3dd-483f-85db-8daa6e9aba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153ccefe-3df1-4ccc-a390-6fc584ef7d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e8eb35-b98b-4f1f-9237-b576432717da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict): \n",
    "    messages: List[HumanMessage]\n",
    "\n",
    "llm =ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16491c89-5437-475f-b17b-ef59b90ce0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(state: AgentState) -> AgentState:\n",
    "    response =llm.invoke(state[\"messages\"])\n",
    "    print(f\"\\nAI:{response.content}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551e0da0-21ec-48b6-99ea-010e43ca3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"process\", process)\n",
    "graph.add_edge(START, \"process\") \n",
    "graph.add_edge(\"process\", END)\n",
    "\n",
    "agent = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23faee74-2ff8-4a39-ab4c-7db73f9b0245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI:Hello! How can I assist you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input= input(\"Enter: \")\n",
    "while user_input!= \"exit\":\n",
    "    agent.invoke({'messages':[HumanMessage(content=user_input)]})\n",
    "    user_input= input(\"Enter: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ee3ea-3632-4323-bdbb-53f88c7dc595",
   "metadata": {},
   "source": [
    "Try giving input and asking the questions. It cannot answer your questions based on your questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43878cad-773a-49a0-a67f-5991aa50581a",
   "metadata": {},
   "source": [
    "## Let's make an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b9bddfa-58fe-4fad-9828-0d072c54b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Dict, Any, Optional, Union\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ccee340-fd4b-4c64-baea-fd28069d4cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb755f0e-8434-43a4-8447-4a3dd65f24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict): \n",
    "    messages: List[Union[HumanMessage, AIMessage]]\n",
    "\n",
    "llm =ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "def process(state: AgentState) -> AgentState:\n",
    "    \"\"\" Thid nod will solve the request you input\"\"\"\n",
    "    response =llm.invoke(state[\"messages\"])\n",
    "    state[\"messages\"].append(AIMessage(content=response.content))\n",
    "    print(f\"\\nAI:{response.content}\")\n",
    "    print(\"CURRENT STATE:\", state[\"messages\"])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b9e58-dd2a-4c05-af7f-94d48ba1754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"process\", process)\n",
    "graph.add_edge(START, \"process\") \n",
    "graph.add_edge(\"process\", END)\n",
    "\n",
    "agent = graph.compile()\n",
    "\n",
    "# starting from here it gets different from the previous one\n",
    "\n",
    "conversation_history=[]\n",
    "\n",
    "user_input= input(\"Enter: \")\n",
    "while user_input!= \"exit\":\n",
    "    conversation_history.append(HumanMessage(content=user_input))\n",
    "    result= agent.invoke({'messages':conversation_history})\n",
    "    conversation_history= result[\"messages\"]\n",
    "    user_input= input(\"Enter: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57a0d2-4cc5-4da0-a9e8-5cad8b641f9e",
   "metadata": {},
   "source": [
    "### with this version if we exit the bot, it is not going to remember our name again when we run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae5d3a-6513-41e1-b9b0-629ab6c8d8e4",
   "metadata": {},
   "source": [
    "Let's improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a13e44-8f85-4cd3-ae75-c95c53336618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Dict, Any, Optional, Union\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebdfea4-dcb1-4d00-9cf1-16a76e7089d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcaeec5-39b6-4fa5-b917-129bd54d36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict): \n",
    "    messages: List[Union[HumanMessage, AIMessage]]\n",
    "\n",
    "llm =ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "def process(state: AgentState) -> AgentState:\n",
    "    \"\"\" Thid nod will solve the request you input\"\"\"\n",
    "    response =llm.invoke(state[\"messages\"])\n",
    "    state[\"messages\"].append(AIMessage(content=response.content))\n",
    "    print(f\"\\nAI:{response.content}\")\n",
    "    print(\"CURRENT STATE:\", state[\"messages\"])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd9dbe-6ca5-4e20-a758-ef14d7cc7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"process\", process)\n",
    "graph.add_edge(START, \"process\") \n",
    "graph.add_edge(\"process\", END)\n",
    "\n",
    "agent = graph.compile()\n",
    "\n",
    "# starting from here it gets different from the previous one\n",
    "\n",
    "conversation_history=[]\n",
    "\n",
    "user_input= input(\"Enter: \")\n",
    "while user_input!= \"exit\":\n",
    "    conversation_history.append(HumanMessage(content=user_input))\n",
    "    result= agent.invoke({'messages':conversation_history})\n",
    "    conversation_history= result[\"messages\"]\n",
    "    user_input= input(\"Enter: \")\n",
    "\n",
    "#this part is saving our conversation as a txt file\n",
    "with open(\"logging.txt\" , \"w\") as file:\n",
    "    file.write(\"Your Conversation Log:\\n\")\n",
    "    for message in conversation_history:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            file.write(f\"You: {message.content}\\n\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            file.write(f\"AI: {message.content}\\n\\n\")\n",
    "        file.write(\"End of Conversation\")\n",
    "print(\"Conversation saved to logging.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa066a1f-0687-4e0c-b5e6-cc80b2427311",
   "metadata": {},
   "source": [
    "### Our current state is going to be longer and longer. And using that many tokens will increase the price for that. We need to me clever about it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2195648-b63d-496f-b2e8-04f95b991d9b",
   "metadata": {},
   "source": [
    "We can write a code like if the number of the human messages exceeds 5 then remove the first message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4440ea-afdb-4dc2-a6e4-d5741ed89703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
